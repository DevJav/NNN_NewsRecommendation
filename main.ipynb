{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 15:57:58.726177: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-21 15:57:58.878279: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-21 15:57:58.878332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-21 15:57:58.899041: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-21 15:57:58.943882: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-21 15:58:00.053381: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.backend import clear_session\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_IS_BEYOND_ACCURACY_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,\n",
    ")\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import (\n",
    "    slice_join_dataframes,\n",
    "    concat_str_columns,\n",
    "    chunk_dataframe,\n",
    "    split_df,\n",
    ")\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "from ebrec.models.newsrec.dataloader import NRMSDataLoader, NRMSDataLoaderPretransform\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms\n",
    "from ebrec.models.newsrec import NRMSModel\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TIMESTAMPS = False\n",
    "SUBSAMPLE_DATASET = True\n",
    "PATH = Path(\"/dtu/blackhole/01/203937/DeepLearning/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if USE_TIMESTAMPS:\n",
    "        df_history = (\n",
    "            pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "            .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL, DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL)\n",
    "            .pipe(\n",
    "                truncate_history,\n",
    "                column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "                history_size=history_size,\n",
    "                padding_value=0,\n",
    "                enable_warning=False,\n",
    "            )\n",
    "            .pipe(\n",
    "                truncate_history,\n",
    "                column=DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,\n",
    "                history_size=history_size,\n",
    "                padding_value=0,\n",
    "                enable_warning=False,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir: NRMS-2024-12-21 15:58:05-352\n",
      "Lenght of df_train: 900\n",
      "Lenght of df_validation: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dtu/blackhole/01/203937/DeepLearning/dl/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init train- and val-dataloader\n",
      "Init model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 15:58:27.666895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38485 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:86:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative_times.shape: (None, 25)\n",
      "Model initialized\n",
      "Fitting model\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 15:58:33.222439: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fb780284590 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-12-21 15:58:33.222464: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2024-12-21 15:58:33.231332: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-21 15:58:33.264642: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734793113.354219 2094187 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 9s 135ms/step - loss: 1.6094 - val_loss: 0.0000e+00\n",
      "Epoch 2/5\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 1.6093 - val_loss: 0.0000e+00\n",
      "Epoch 3/5\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 1.6090 - val_loss: 0.0000e+00\n",
      "Epoch 4/5\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 1.6080 - val_loss: 0.0000e+00\n",
      "Epoch 5/5\n",
      "29/29 [==============================] - 1s 31ms/step - loss: 1.6039 - val_loss: 0.0000e+00\n",
      "Model fitted\n",
      "saving model: ebnerd_predictions/state_dict/NRMS-2024-12-21 15:58:05-352/weights\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DUMP_DIR = Path(\"ebnerd_predictions\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "SEED = np.random.randint(0, 1_000)\n",
    "\n",
    "MODEL_NAME = f\"NRMS-{dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}-{SEED}\"\n",
    "\n",
    "MODEL_WEIGHTS = DUMP_DIR.joinpath(f\"state_dict/{MODEL_NAME}/weights\")\n",
    "LOG_DIR = DUMP_DIR.joinpath(f\"runs/{MODEL_NAME}\")\n",
    "TEST_DF_DUMP = DUMP_DIR.joinpath(\"test_predictions\", MODEL_NAME)\n",
    "TEST_DF_DUMP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Dir: {MODEL_NAME}\")\n",
    "\n",
    "DATASPLIT = \"ebnerd_small\"\n",
    "MAX_TITLE_LENGTH = 30 \n",
    "HISTORY_SIZE = 25 #number of articles that i count as already seen by user. So if 0 -> it's because we're missing data\n",
    "FRACTION = 1.0\n",
    "EPOCHS = 5\n",
    "FRACTION_TEST = 1.0\n",
    "#\n",
    "hparams_nrms.history_size = HISTORY_SIZE\n",
    "\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "BATCH_SIZE_VAL = 32\n",
    "BATCH_SIZE_TEST_WO_B = 32\n",
    "BATCH_SIZE_TEST_W_B = 4\n",
    "N_CHUNKS_TEST = 10\n",
    "CHUNKS_DONE = 0\n",
    "\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "]\n",
    "\n",
    "if USE_TIMESTAMPS:\n",
    "    COLUMNS.append(DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL)\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .sample(fraction=FRACTION)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    ")\n",
    "\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "\n",
    "if SUBSAMPLE_DATASET:\n",
    "    df_train = df_train[:1000]\n",
    "    df_train, df_validation = split_df(df_train, fraction=0.9, seed=SEED, shuffle=False)\n",
    "\n",
    "df_articles = pl.read_parquet(PATH.joinpath(DATASPLIT, \"articles.parquet\"))\n",
    "\n",
    "print(f\"Lenght of df_train: {len(df_train)}\")\n",
    "print(f\"Lenght of df_validation: {len(df_validation)}\")\n",
    "\n",
    "# =>\n",
    "TRANSFORMER_MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE) # concat subtitle and title, cat_cal is the column name\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")\n",
    "\n",
    "# =>\n",
    "print(\"Init train- and val-dataloader\")\n",
    "if USE_TIMESTAMPS:\n",
    "    train_dataloader = NRMSDataLoader(\n",
    "        behaviors=df_train,\n",
    "        article_dict=article_mapping,\n",
    "        unknown_representation=\"zeros\",\n",
    "        history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "        timestamp_column=DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,\n",
    "        eval_mode=False,\n",
    "        batch_size=BATCH_SIZE_TRAIN,\n",
    "    )\n",
    "    val_dataloader = NRMSDataLoader(\n",
    "        behaviors=df_validation,\n",
    "        article_dict=article_mapping,\n",
    "        unknown_representation=\"zeros\",\n",
    "        history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "        timestamp_column=DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL,\n",
    "        eval_mode=True,\n",
    "        batch_size=BATCH_SIZE_VAL,\n",
    "    )\n",
    "else:\n",
    "    train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BATCH_SIZE_TRAIN,\n",
    "    )\n",
    "    val_dataloader = NRMSDataLoader(\n",
    "        behaviors=df_validation,\n",
    "        article_dict=article_mapping,\n",
    "        unknown_representation=\"zeros\",\n",
    "        history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "        eval_mode=True,\n",
    "        batch_size=BATCH_SIZE_VAL,\n",
    "    )\n",
    "\n",
    "print(\"Init model\")\n",
    "model = NRMSModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "print(\"Model initialized\")\n",
    "\n",
    "print(\"Fitting model\")\n",
    "hist = model.model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=EPOCHS,\n",
    ")\n",
    "print(\"Model fitted\")\n",
    "\n",
    "print(f\"saving model: {MODEL_WEIGHTS}\")\n",
    "model.model.save_weights(MODEL_WEIGHTS)\n",
    "# print(f\"loading model: {MODEL_WEIGHTS}\")\n",
    "# model.model.load_weights(MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 30ms/step\n",
      "<MetricEvaluator class>: \n",
      " {\n",
      "    \"auc\": 0.535,\n",
      "    \"mrr\": 0.49500000000000005,\n",
      "    \"ndcg@5\": 0.6186968876858397,\n",
      "    \"ndcg@10\": 0.6186968876858397\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pred_validation = model.scorer.predict(val_dataloader)\n",
    "df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "    add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    ")\n",
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "print(metrics.evaluate())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
